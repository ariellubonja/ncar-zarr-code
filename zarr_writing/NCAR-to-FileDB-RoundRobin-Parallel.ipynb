{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all netCDF NCAR timestep files to Zarr 512 arrays, with Grouped Velocity components, with (64,64,64) chunk size, round-robined across FileDB nodes (spatially using Z-order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Old Dask version gives this error https://github.com/dask/distributed/issues/3955</font>\n",
    "\n",
    "<font color='orange'>Note: Careful when Setting Dask `local_directory` to remote server (e.g. Temporary) will HUGELY slow down functions</font>\n",
    "\n",
    "<font color='cyan'>Parallel version needs Large job</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cube_side = 512\n",
    "chunk_size = 64\n",
    "raw_ncar_folder_path = '/home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt'\n",
    "use_dask = True\n",
    "dest_folder_name = \"sabl2048b\" # B is the high-rate data\n",
    "write_type = \"prod\" # or \"back\"\n",
    "\n",
    "n_dask_workers = 4 # For Dask rechunking\n",
    "dask_local_dir = '/home/idies/workspace/turb/data02_02'\n",
    "\n",
    "n_pool_workers = 4 # For writing to disk\n",
    "\n",
    "timestep_nr = 0\n",
    "# timestep_range = range(1) # Ned's new High-rate fixed-dt has only 5 timesteps\n",
    "# timestep_range = range(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"dask[complete]\"\n",
    "# !pip install \"xarray[complete]\"\n",
    "# !pip install morton-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from utils import write_tools\n",
    "import dask\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get target Folder list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=write_tools.list_fileDB_folders()\n",
    "\n",
    "# Avoiding 7-2 and 9-2 - they're too full as of May 2023\n",
    "folders.remove(\"/home/idies/workspace/turb/data09_02/zarr/\")\n",
    "folders.remove(\"/home/idies/workspace/turb/data07_02/zarr/\")\n",
    "\n",
    "for i in range(len(folders)):\n",
    "    folders[i] += dest_folder_name + \"_\" + str(i + 1).zfill(2) + \"_\" + write_type + \"/\"\n",
    "\n",
    "# folders[:5]\n",
    "\n",
    "# Create top-level dirs\n",
    "\n",
    "# for folder_path in folders:\n",
    "#     os.makedirs(folder_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Don't delete the CD cell!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preparing data. Starting to write...\n"
     ]
    }
   ],
   "source": [
    "# for timestep_nr in timestep_range:\n",
    "data_xr = xr.open_dataset(raw_ncar_folder_path + \"/jhd.\" + str(timestep_nr).zfill(3) + \".nc\")\n",
    "\n",
    "# Group 3 velocity components together\n",
    "# This fails with Dask bcs. of write permission error on SciServer Job\n",
    "# Never use dask with remote location on this!!\n",
    "merged_velocity = write_tools.merge_velocities(data_xr, dask_local_dir=dask_local_dir\n",
    "                                               , chunk_size_base=chunk_size, use_dask=True, n_dask_workers=n_dask_workers)\n",
    "\n",
    "\n",
    "# Unabbreviate 'e', 'p', 't' variable names\n",
    "merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})\n",
    "\n",
    "\n",
    "dims = [dim for dim in data_xr.dims]\n",
    "dims.reverse() # use (nnz, nny, nnx) instead of (nnx, nny, nnz)\n",
    "\n",
    "# Split 2048^3 into smaller 512^3 arrays\n",
    "smaller_groups, _ = write_tools.split_zarr_group(merged_velocity, desired_cube_side, dims)\n",
    "\n",
    "# Given up in favor of Ryan's node coloring technique\n",
    "#     z_order = write_tools.morton_order_cube(cube_side=4)\n",
    "node_assignments = write_tools.node_assignment(cube_side=4)\n",
    "\n",
    "\n",
    "cubes = smaller_groups\n",
    "\n",
    "encoding={\n",
    "    \"velocity\": dict(chunks=(chunk_size, chunk_size, chunk_size, 3), compressor=None),\n",
    "    \"pressure\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "    \"temperature\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "    \"energy\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None)\n",
    "}\n",
    "\n",
    "print('Done preparing data. Starting to write...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New method took only 15min to MergeVelocity!\n",
    "\n",
    "Maybe save merged_velocity to file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute 512^3 across FileDB\n",
    "\n",
    "# tasks = []\n",
    "# for i in range(len(cubes)):\n",
    "#     for j in range(len(cubes[i])):\n",
    "#         for k in range(len(cubes[i][j])):\n",
    "#             filedb_index = node_assignments[i][j][k]# % len(folders) # ryan's node assignment accounts for nr. nodes on filedb\n",
    "#             current_array = cubes[i][j][k]\n",
    "\n",
    "#             chunk_nr = 16 * i + 4 * j + k\n",
    "\n",
    "#             # turb/data02_02/sabl2048b_prod/ + sabl2048b + 05 + _ + 001.zarr\n",
    "#             dest_groupname = os.path.join(folders[filedb_index - 1], dest_folder_name + str(chunk_nr + 1).zfill(2) + \"_\" + str(timestep_nr).zfill(3) + \".zarr\")\n",
    "            \n",
    "#             write_tools.write_to_disk(dest_groupname, current_array, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "def write_to_disk(q):\n",
    "    while True:\n",
    "        try:\n",
    "            chunk, dest_groupname, encoding = q.get(timeout=10)  # Adjust timeout as necessary\n",
    "            chunk.to_zarr(store=dest_groupname, mode=\"w\", encoding=encoding)\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        finally:\n",
    "            q.task_done()\n",
    "\n",
    "# Create a queue\n",
    "q = queue.Queue()\n",
    "\n",
    "# Populate the queue with tasks\n",
    "for i in range(len(cubes)):\n",
    "    for j in range(len(cubes[i])):\n",
    "        for k in range(len(cubes[i][j])):\n",
    "            filedb_index = node_assignments[i][j][k]\n",
    "            current_array = cubes[i][j][k]\n",
    "            dest_groupname = os.path.join(folders[filedb_index - 1], dest_folder_name + str(node_assignments[i][j][k]).zfill(2) + \"_\" + str(timestep_nr).zfill(3) + \".zarr\")\n",
    "            q.put((current_array, dest_groupname, encoding))\n",
    "\n",
    "# Create threads and start them\n",
    "num_threads = 8  # Adjust based on the number of cores and expected I/O operations\n",
    "threads = []\n",
    "for _ in range(num_threads):\n",
    "    t = threading.Thread(target=write_to_disk, args=(q,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Wait for all tasks to be processed\n",
    "q.join()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def write_task(args):\n",
    "    i, j, k, node_assignments, cubes, folders, dest_folder_name, timestep_nr, encoding = args\n",
    "\n",
    "    filedb_index = node_assignments[i][j][k]\n",
    "    current_array = cubes[i][j][k]\n",
    "    \n",
    "    chunk_nr = 16 * i + 4 * j + k\n",
    "    \n",
    "    dest_groupname = os.path.join(folders[filedb_index - 1], dest_folder_name + str(chunk_nr + 1).zfill(2) + \"_\" + str(timestep_nr).zfill(3) + \".zarr\")\n",
    "    \n",
    "\n",
    "    # Assuming write_tools.write_to_disk is imported\n",
    "    write_tools.write_to_disk(dest_groupname, current_array, encoding)\n",
    "\n",
    "\n",
    "tasks = []\n",
    "for i in range(len(cubes)):\n",
    "    for j in range(len(cubes[i])):\n",
    "        for k in range(len(cubes[i][j])):\n",
    "            tasks.append((i, j, k, node_assignments, cubes, folders, dest_folder_name, timestep_nr, encoding))\n",
    "\n",
    "# Create a multiprocessing pool with 4 workers\n",
    "with Pool(n_pool_workers) as pool:\n",
    "    pool.map(write_task, tasks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
