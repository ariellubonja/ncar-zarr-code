{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all netCDF NCAR timestep files to Zarr 512 arrays, with Grouped Velocity components, with (64,64,64) chunk size, round-robined across FileDB nodes (spatially using Z-order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Old Dask version gives this error https://github.com/dask/distributed/issues/3955</font>\n",
    "\n",
    "<font color='orange'>Note: Careful when Setting Dask `local_directory` to remote server (e.g. Temporary) will HUGELY slow down functions</font>\n",
    "\n",
    "<font color='cyan'>Parallel version needs Large job</font>\n",
    "\n",
    "<font color = 'gold'>TODO fix MemoryError: Unable to allocate 32.0 GiB for an array with shape (2048, 2048, 2048) and data type float32 when looping over multiple timesteps</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cube_side = 512\n",
    "chunk_size = 64\n",
    "raw_ncar_folder_path = '/home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt'\n",
    "use_dask = True\n",
    "dest_folder_name = \"sabl2048b\" # B is the high-rate data\n",
    "write_type = \"prod\" # or \"back\" for backup\n",
    "\n",
    "n_dask_workers = 4 # For Dask rechunking\n",
    "num_threads = 34  # For writing to FileDB\n",
    "dask_local_dir = '/home/idies/workspace/turb/data02_02'\n",
    "\n",
    "\n",
    "timestep_nr = 2\n",
    "# timestep_range = range(1) # This doesn't work with MemoryError: Unable to allocate 32.0 GiB for an array with shape (2048, 2048, 2048) and data type float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from utils import write_tools\n",
    "import dask\n",
    "import os\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get target Folder list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=write_tools.list_fileDB_folders()\n",
    "\n",
    "# Avoiding 7-2 and 9-2 - they're too full as of May 2023\n",
    "folders.remove(\"/home/idies/workspace/turb/data09_02/zarr/\")\n",
    "folders.remove(\"/home/idies/workspace/turb/data07_02/zarr/\")\n",
    "\n",
    "for i in range(len(folders)):\n",
    "    folders[i] += dest_folder_name + \"_\" + str(i + 1).zfill(2) + \"_\" + write_type + \"/\"\n",
    "\n",
    "# folders[:5]\n",
    "\n",
    "# Create top-level dirs\n",
    "\n",
    "# for folder_path in folders:\n",
    "#     os.makedirs(folder_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Don't delete the CD cell!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for timestep_nr in timestep_range:\n",
    "data_xr = xr.open_dataset(raw_ncar_folder_path + \"/jhd.\" + str(timestep_nr).zfill(3) + \".nc\")\n",
    "\n",
    "# Group 3 velocity components together\n",
    "# This fails with Dask bcs. of write permission error on SciServer Job\n",
    "# Never use dask with remote location on this!!\n",
    "merged_velocity = write_tools.merge_velocities(data_xr, dask_local_dir=dask_local_dir\n",
    "                                               , chunk_size_base=chunk_size, use_dask=True, n_dask_workers=n_dask_workers)\n",
    "\n",
    "\n",
    "# Unabbreviate 'e', 'p', 't' variable names\n",
    "merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})\n",
    "\n",
    "\n",
    "dims = [dim for dim in data_xr.dims]\n",
    "dims.reverse() # use (nnz, nny, nnx) instead of (nnx, nny, nnz)\n",
    "\n",
    "# Split 2048^3 into smaller 512^3 arrays\n",
    "smaller_groups, _ = write_tools.split_zarr_group(merged_velocity, desired_cube_side, dims)\n",
    "\n",
    "# Given up in favor of Ryan's node coloring technique\n",
    "#     z_order = write_tools.morton_order_cube(cube_side=4)\n",
    "node_assignments = write_tools.node_assignment(cube_side=4)\n",
    "\n",
    "\n",
    "cubes = smaller_groups\n",
    "\n",
    "encoding={\n",
    "    \"velocity\": dict(chunks=(chunk_size, chunk_size, chunk_size, 3), compressor=None),\n",
    "    \"pressure\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "    \"temperature\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "    \"energy\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None)\n",
    "}\n",
    "\n",
    "print('Done preparing data. Starting to write...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_disk(q):\n",
    "    while True:\n",
    "        try:\n",
    "            chunk, dest_groupname, encoding = q.get(timeout=10)  # Adjust timeout as necessary\n",
    "            \n",
    "            print(f\"Starting write to {dest_groupname}...\")\n",
    "            chunk.to_zarr(store=dest_groupname, mode=\"w\", encoding=encoding)\n",
    "            print(f\"Finished writing to {dest_groupname}.\")\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        finally:\n",
    "            q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = queue.Queue()\n",
    "\n",
    "\n",
    "# Populate the queue with tasks\n",
    "for i in range(len(cubes)):\n",
    "    for j in range(len(cubes[i])):\n",
    "        for k in range(len(cubes[i][j])):\n",
    "            filedb_index = node_assignments[i][j][k]\n",
    "            current_array = cubes[i][j][k]\n",
    "\n",
    "            chunk_nr = str(i * 16 + j * 4 + k + 1).zfill(2)\n",
    "\n",
    "            dest_groupname = os.path.join(folders[filedb_index - 1], dest_folder_name + chunk_nr + \"_\" + str(timestep_nr).zfill(3) + \".zarr\")\n",
    "            q.put((current_array, dest_groupname, encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create threads and start them\n",
    "\n",
    "threads = []\n",
    "for _ in range(num_threads):\n",
    "    t = threading.Thread(target=write_to_disk, args=(q,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Wait for all tasks to be processed\n",
    "q.join()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cool hack to restart the kernel!\n",
    "\n",
    "!kill -9 `pgrep -f \"kernel-.*.json\"` && echo \"Kernel restarted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
