{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all netCDF NCAR timestep files to Zarr 512 arrays, with Grouped Velocity components, with (64,64,64) chunk size, round-robined across FileDB nodes (spatially using Z-order)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Old Dask version gives this error https://github.com/dask/distributed/issues/3955</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cube_side = 512\n",
    "chunk_size = 64\n",
    "raw_ncar_folder_path = '/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF'\n",
    "use_dask = False # Has issues on SciServer Compute\n",
    "dest_folder_name = \"sabl2048b\" # B is the high-rate data\n",
    "type = \"prod\" # or \"back\"\n",
    "# timestep_nr = 1\n",
    "timestep_range = range(5) # Ned's new High-rate fixed-dt has only 5 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"dask[complete]\"\n",
    "!pip install \"xarray[complete]\"\n",
    "!pip install morton-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from utils import write_tools\n",
    "import dask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get target Folder list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/idies/workspace/turb/data01_01/zarr/sabl2048b_01_prod/',\n",
       " '/home/idies/workspace/turb/data02_01/zarr/sabl2048b_02_prod/',\n",
       " '/home/idies/workspace/turb/data03_01/zarr/sabl2048b_03_prod/',\n",
       " '/home/idies/workspace/turb/data04_01/zarr/sabl2048b_04_prod/',\n",
       " '/home/idies/workspace/turb/data05_01/zarr/sabl2048b_05_prod/']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders=write_tools.list_fileDB_folders()\n",
    "\n",
    "# Avoiding 7-2 and 9-2 - they're too full as of May 2023\n",
    "folders.remove(\"/home/idies/workspace/turb/data09_02/zarr/\")\n",
    "folders.remove(\"/home/idies/workspace/turb/data07_02/zarr/\")\n",
    "\n",
    "for i in range(len(folders)):\n",
    "    folders[i] += dest_folder_name + \"_\" + str(i + 1).zfill(2) + \"_\" + type + \"/\"\n",
    "\n",
    "folders[:5]\n",
    "\n",
    "# Create top-level dirs\n",
    "\n",
    "# for folder_path in folders:\n",
    "#     os.makedirs(folder_path, exist_ok=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Don't delete the CD cell!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "[Errno 2] No such file or directory: '/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF/jhd.000.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/file_manager.py:210\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key]\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache[key]\n\u001b[1;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<function _open_scipy_netcdf at 0x128628670>, ('/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF/jhd.000.nc',), 'r', (('mmap', None), ('version', 2)), 'd4efa538-8a04-4fb5-9e9c-548168527edb']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m timestep_nr \u001b[39min\u001b[39;00m timestep_range:\n\u001b[0;32m----> 2\u001b[0m     data_xr \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39;49mopen_dataset(\u001b[39m\"\u001b[39;49m\u001b[39m/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF/jhd.00\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(timestep_nr) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.nc\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Group 3 velocity components together\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# This fails with Dask bcs. of write permission error on SciServer Job\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     merged_velocity \u001b[39m=\u001b[39m write_tools\u001b[39m.\u001b[39mmerge_velocities(data_xr, chunk_size_base\u001b[39m=\u001b[39mchunk_size, use_dask\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/api.py:566\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m decoders \u001b[39m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    555\u001b[0m     decode_cf,\n\u001b[1;32m    556\u001b[0m     open_backend_dataset_parameters\u001b[39m=\u001b[39mbackend\u001b[39m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     decode_coords\u001b[39m=\u001b[39mdecode_coords,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m overwrite_encoded_chunks \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39moverwrite_encoded_chunks\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 566\u001b[0m backend_ds \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mopen_dataset(\n\u001b[1;32m    567\u001b[0m     filename_or_obj,\n\u001b[1;32m    568\u001b[0m     drop_variables\u001b[39m=\u001b[39;49mdrop_variables,\n\u001b[1;32m    569\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdecoders,\n\u001b[1;32m    570\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    572\u001b[0m ds \u001b[39m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    573\u001b[0m     backend_ds,\n\u001b[1;32m    574\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    586\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:315\u001b[0m, in \u001b[0;36mScipyBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, mode, format, group, mmap, lock)\u001b[0m\n\u001b[1;32m    313\u001b[0m store_entrypoint \u001b[39m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    314\u001b[0m \u001b[39mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 315\u001b[0m     ds \u001b[39m=\u001b[39m store_entrypoint\u001b[39m.\u001b[39;49mopen_dataset(\n\u001b[1;32m    316\u001b[0m         store,\n\u001b[1;32m    317\u001b[0m         mask_and_scale\u001b[39m=\u001b[39;49mmask_and_scale,\n\u001b[1;32m    318\u001b[0m         decode_times\u001b[39m=\u001b[39;49mdecode_times,\n\u001b[1;32m    319\u001b[0m         concat_characters\u001b[39m=\u001b[39;49mconcat_characters,\n\u001b[1;32m    320\u001b[0m         decode_coords\u001b[39m=\u001b[39;49mdecode_coords,\n\u001b[1;32m    321\u001b[0m         drop_variables\u001b[39m=\u001b[39;49mdrop_variables,\n\u001b[1;32m    322\u001b[0m         use_cftime\u001b[39m=\u001b[39;49muse_cftime,\n\u001b[1;32m    323\u001b[0m         decode_timedelta\u001b[39m=\u001b[39;49mdecode_timedelta,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/store.py:43\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_dataset\u001b[39m(  \u001b[39m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     filename_or_obj: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m os\u001b[39m.\u001b[39mPathLike[Any] \u001b[39m|\u001b[39m BufferedIOBase \u001b[39m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     decode_timedelta\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[1;32m     41\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(filename_or_obj, AbstractDataStore)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mvars\u001b[39m, attrs \u001b[39m=\u001b[39m filename_or_obj\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     44\u001b[0m     encoding \u001b[39m=\u001b[39m filename_or_obj\u001b[39m.\u001b[39mget_encoding()\n\u001b[1;32m     46\u001b[0m     \u001b[39mvars\u001b[39m, attrs, coord_names \u001b[39m=\u001b[39m conventions\u001b[39m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     47\u001b[0m         \u001b[39mvars\u001b[39m,\n\u001b[1;32m     48\u001b[0m         attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         decode_timedelta\u001b[39m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     56\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/common.py:132\u001b[0m, in \u001b[0;36mAbstractDataStore.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    This loads the variables and attributes simultaneously.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    A centralized loading function makes it easier to create\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m    are requested, so care should be taken to make sure its fast.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     variables \u001b[39m=\u001b[39m FrozenDict(\n\u001b[0;32m--> 132\u001b[0m         (_decode_variable_name(k), v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_variables()\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m     attributes \u001b[39m=\u001b[39m FrozenDict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_attrs())\n\u001b[1;32m    135\u001b[0m     \u001b[39mreturn\u001b[39;00m variables, attributes\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:181\u001b[0m, in \u001b[0;36mScipyDataStore.get_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_variables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m FrozenDict(\n\u001b[0;32m--> 181\u001b[0m         (k, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen_store_variable(k, v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds\u001b[39m.\u001b[39mvariables\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    182\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:170\u001b[0m, in \u001b[0;36mScipyDataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mds\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_manager\u001b[39m.\u001b[39;49macquire()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/file_manager.py:192\u001b[0m, in \u001b[0;36mCachingFileManager.acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39macquire\u001b[39m(\u001b[39mself\u001b[39m, needs_lock\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    178\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Acquire a file object from the manager.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[39m    A new file is only opened if it has expired from the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m        An open file object, as returned by ``opener(*args, **kwargs)``.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     file, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/file_manager.py:216\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    214\u001b[0m     kwargs \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    215\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode\n\u001b[0;32m--> 216\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opener(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    218\u001b[0m     \u001b[39m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:109\u001b[0m, in \u001b[0;36m_open_scipy_netcdf\u001b[0;34m(filename, mode, mmap, version)\u001b[0m\n\u001b[1;32m    106\u001b[0m     filename \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(filename)\n\u001b[1;32m    108\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m scipy\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mnetcdf_file(filename, mode\u001b[39m=\u001b[39;49mmode, mmap\u001b[39m=\u001b[39;49mmmap, version\u001b[39m=\u001b[39;49mversion)\n\u001b[1;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# netcdf3 message is obscure in this case\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     errmsg \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/io/_netcdf.py:246\u001b[0m, in \u001b[0;36mnetcdf_file.__init__\u001b[0;34m(self, filename, mode, mmap, version, maskandscale)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m filename\n\u001b[1;32m    245\u001b[0m omode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m mode\n\u001b[0;32m--> 246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, \u001b[39m'\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m omode)\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m mmap \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# Mmapped files on PyPy cannot be usually closed\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39m# before the GC runs, so it's better to use mmap=False\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[39m# as the default.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     mmap \u001b[39m=\u001b[39m (\u001b[39mnot\u001b[39;00m IS_PYPY)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF/jhd.000.nc'"
     ]
    }
   ],
   "source": [
    "for timestep_nr in timestep_range:\n",
    "    data_xr = xr.open_dataset(\"/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF/jhd.00\" + str(timestep_nr) + \".nc\")\n",
    "    \n",
    "    # Group 3 velocity components together\n",
    "    # This fails with Dask bcs. of write permission error on SciServer Job\n",
    "    merged_velocity = write_tools.merge_velocities(data_xr, chunk_size_base=chunk_size, use_dask=False)\n",
    "    \n",
    "    # Unabbreviate 'e', 'p', 't' variable names\n",
    "    merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})\n",
    "    \n",
    "    # Split 2048^3 into smaller 512^3 arrays\n",
    "    dims = [dim for dim in data_xr.dims]\n",
    "    dims.reverse() # use (nnz, nny, nnx) instead of (nnx, nny, nnz)\n",
    "    \n",
    "    smaller_groups = write_tools.split_zarr_group(merged_velocity, desired_cube_side, dims)\n",
    "    \n",
    "    # Given up in favor of Ryan's node coloring technique\n",
    "#     z_order = write_tools.morton_order_cube(cube_side=4)\n",
    "    \n",
    "    node_assignments = write_tools.node_assignment(cube_side=4)\n",
    "    \n",
    "    \n",
    "    # Distribute them across FileDB\n",
    "    cubes = smaller_groups\n",
    "    \n",
    "    encoding={\n",
    "        \"velocity\": dict(chunks=(chunk_size, chunk_size, chunk_size, 3), compressor=None),\n",
    "        \"pressure\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "        \"temperature\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "        \"energy\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None)\n",
    "    }\n",
    "    \n",
    "    tasks = []\n",
    "    for i in range(len(cubes)):\n",
    "        for j in range(len(cubes[i])):\n",
    "            for k in range(len(cubes[i][j])):\n",
    "                filedb_index = node_assignments[i][j][k]# % len(folders) # ryan's node assignment accounts for nr. nodes on filedb\n",
    "                current_array = cubes[i][j][k]\n",
    "                \n",
    "                # turb/data02_02/sabl2048b_prod/ + sabl2048b + 05 + _ + 001.zarr\n",
    "                dest_groupname = folders[filedb_index - 1] + dest_folder_name + str(node_assignments[i][j][k]).zfill(2) + \"_\" + str(timestep_nr).zfill(3) + \".zarr\"\n",
    "\n",
    "                # print(dest_groupname)\n",
    "                \n",
    "                if use_dask:\n",
    "                    tasks.append(write_tools.write_to_disk_dask(dest_groupname, current_array, encoding))\n",
    "                else:\n",
    "                    write_tools.write_to_disk(dest_groupname, current_array, encoding)\n",
    "\n",
    "    if use_dask:\n",
    "        dask.compute(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
