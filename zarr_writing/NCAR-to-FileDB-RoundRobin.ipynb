{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Automate this for all NCAR files. Now it works for just 1st file\n",
    "\n",
    "<font color=\"red\">Dask everything</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all netCDF NCAR timestep files to Zarr 512 arrays, with Grouped Velocity components, with (64,64,64) chunk size, round-robined across FileDB nodes (spatially using Z-order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cube_side = 512\n",
    "chunk_size = 64\n",
    "raw_ncar_folder_path = '/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF'\n",
    "# Has issues on SciServer Compute\n",
    "use_dask = True\n",
    "dest_folder_name = \"sabl2048a\"\n",
    "timestep_nr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"dask[complete]\"\n",
    "!pip install \"xarray[complete]\"\n",
    "!pip install morton-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import write_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Don't delete the CD cell!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xr = xr.open_dataset(\"/home/idies/workspace/turb/data02_02/ariel-6-timestep-ncar-netCDF/jhd.00\" + str(timestep_nr) + \".nc\")\n",
    "# data_xr.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 3 velocity components together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Old Dask version gives this error https://github.com/dask/distributed/issues/3955</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Testing purposes - make data much smaller - faster\n",
    "\n",
    "# desired_cube_side = 64\n",
    "# smaller_size = 128\n",
    "# i=0\n",
    "# j=0\n",
    "# k=0\n",
    "# dims = [dim for dim in data_xr.dims]\n",
    "\n",
    "# data_xr = data_xr.isel( # TODO make this into a function: slice_group()\n",
    "#     {dims[0]: slice(i * smaller_size, (i + 1) * smaller_size),\n",
    "#      dims[1]: slice(j * smaller_size, (j + 1) * smaller_size),\n",
    "#      dims[2]: slice(k * smaller_size, (k + 1) * smaller_size)}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_velocity = write_tools.merge_velocities(data_xr, chunk_size_base=chunk_size, use_dask=use_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unabbreviate 'e', 'p', 't' variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split 2048^3 into smaller 512^3 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [dim for dim in data_xr.dims]\n",
    "smaller_groups = write_tools.split_zarr_group(merged_velocity, desired_cube_side, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller_groups should be a cube (list of lists of lists)\n",
    "# smaller_groups[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-order the smaller Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 4], [2, 6]], [[1, 5], [3, 7]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_order = write_tools.morton_order_cubes(smaller_groups)\n",
    "# z_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribute them across FileDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get target Folder list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/idies/workspace/turb/data01_01/zarr/sabl2048a_01_prod',\n",
       " '/home/idies/workspace/turb/data02_01/zarr/sabl2048a_02_prod',\n",
       " '/home/idies/workspace/turb/data03_01/zarr/sabl2048a_03_prod',\n",
       " '/home/idies/workspace/turb/data04_01/zarr/sabl2048a_04_prod',\n",
       " '/home/idies/workspace/turb/data05_01/zarr/sabl2048a_05_prod']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders=write_tools.list_fileDB_folders()\n",
    "\n",
    "# Avoiding 7-2 and 9-2 - they're too full as of May 2023\n",
    "# folders.remove(\"/home/idies/workspace/turb/data02_02/zarr/ncar-zarr/\")\n",
    "folders.remove(\"/home/idies/workspace/turb/data09_02/zarr/\")\n",
    "folders.remove(\"/home/idies/workspace/turb/data07_02/zarr/\") # This is already created\n",
    "\n",
    "for i in range(len(folders)):\n",
    "    folders[i] += dest_folder_name + \"_\" + str(i + 1).zfill(2) + \"_prod\"\n",
    "\n",
    "folders[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make top-level dir\n",
    "\n",
    "# for folder_path in folders:\n",
    "#     os.makedirs(folder_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None, None, None, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "cubes = smaller_groups\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def write_to_disk(z_order_nr, current_array, filedb_index):\n",
    "    encoding={\n",
    "        \"velocity\": dict(chunks=(chunk_size, chunk_size, chunk_size, 3), compressor=None),\n",
    "        \"pressure\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "        \"temperature\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None),\n",
    "        \"energy\": dict(chunks=(chunk_size, chunk_size, chunk_size, 1), compressor=None)\n",
    "    }\n",
    "#     encoding = {variable_name: {'compressor': None, 'chunks': (chunk_size, chunk_size, chunk_size)} for variable_name in current_array.variables}\n",
    "    \n",
    "    dest_groupname = folders[filedb_index] + dest_folder_name + str(z_order_nr).zfill(2) + \"_\" + str(timestep_nr).zfill(2) + \".zarr\"\n",
    "    \n",
    "    current_array.to_zarr(store=dest_groupname,\n",
    "        mode=\"w\",\n",
    "        encoding = encoding)\n",
    "\n",
    "\n",
    "tasks = []\n",
    "for i in range(len(cubes)):\n",
    "    for j in range(len(cubes[i])):\n",
    "        for k in range(len(cubes[i][j])):\n",
    "            # Put e.g. z-order=4 to filedb05_01\n",
    "            filedb_index = z_order[i][j][k] % len(folders) # Loop around\n",
    "            current_array = cubes[i][j][k]\n",
    "            if use_dask:\n",
    "                tasks.append(write_to_disk(z_order[i][j][k], current_array, filedb_index))\n",
    "            else:\n",
    "                write_to_disk(z_order[i][j][k], current_array, filedb_index)\n",
    "\n",
    "if use_dask:\n",
    "    dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "\n",
    "# # Possible Parallel implementation issue:\n",
    "# # Ariel: possible contention on data_xr = xr.open_dataset(file_name)\n",
    "# #     bcs all source data live on data02_02\n",
    "# for file_name in os.listdir(raw_ncar_folder_path):\n",
    "#     if os.path.isfile(os.path.join(raw_ncar_folder_path, file_name)):\n",
    "#         # https://github.com/pangeo-data/pangeo/issues/150\n",
    "#         data_xr = xr.open_dataset(file_name)\n",
    "\n",
    "#         # Disable compression, set chunk size\n",
    "#         encoding = {variable_name: {'compressor': None, 'chunks': (chunk_size_base, chunk_size_base, chunk_size_base)} for variable_name in data_xr.variables}\n",
    "        \n",
    "#         target_dir = folders[i]\n",
    "\n",
    "#         # overwrite if exists\n",
    "#         data_xr.to_zarr(store=target_dir + \"ncar_\" + str(i) + \"_\" + \"chunk_\" + str(chunk_size_base) + \".zarr\",\n",
    "#                         mode=\"w\",\n",
    "#                        encoding = encoding)\n",
    "        \n",
    "#         print(file_name)\n",
    "#         i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
