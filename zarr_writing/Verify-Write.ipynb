{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all netCDF NCAR timestep files to Zarr 512 arrays, with Grouped Velocity components, with (64,64,64) chunk size, round-robined across FileDB nodes (spatially using Z-order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Old Dask version gives this error https://github.com/dask/distributed/issues/3955</font>\n",
    "\n",
    "<font color='orange'>Note: Careful when Setting Dask `local_directory` to remote server (e.g. Temporary) will HUGELY slow down functions</font>\n",
    "\n",
    "<font color='cyan'>Parallel version needs Large job</font>\n",
    "\n",
    "<font color = 'gold'>TODO fix MemoryError: Unable to allocate 32.0 GiB for an array with shape (2048, 2048, 2048) and data type float32 when looping over multiple timesteps</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cube_side = 512\n",
    "chunk_size = 64\n",
    "raw_ncar_folder_path = '/home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt'\n",
    "use_dask = True\n",
    "dest_folder_name = \"sabl2048b\" # B is the high-rate data\n",
    "write_type = \"prod\" # or \"back\" for backup\n",
    "\n",
    "n_dask_workers = 4 # For Dask rechunking\n",
    "num_threads = 34  # For writing to FileDB\n",
    "dask_local_dir = '/home/idies/workspace/turb/data02_02'\n",
    "\n",
    "\n",
    "timestep_nr = 0\n",
    "# timestep_range = range(1) # This doesn't work with MemoryError: Unable to allocate 32.0 GiB for an array with shape (2048, 2048, 2048) and data type float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/Storage/ariel4/persistent/ncar-zarr-code/zarr_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from utils import write_tools\n",
    "import os\n",
    "import threading\n",
    "import queue\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get target Folder list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=write_tools.list_fileDB_folders()\n",
    "\n",
    "# Avoiding 7-2 and 9-2 - they're too full as of May 2023\n",
    "folders.remove(\"/home/idies/workspace/turb/data09_02/zarr/\")\n",
    "folders.remove(\"/home/idies/workspace/turb/data07_02/zarr/\")\n",
    "\n",
    "for i in range(len(folders)):\n",
    "    folders[i] += dest_folder_name + \"_\" + str(i + 1).zfill(2) + \"_\" + write_type + \"/\"\n",
    "\n",
    "# folders[:5]\n",
    "\n",
    "# Create top-level dirs\n",
    "\n",
    "# for folder_path in folders:\n",
    "#     os.makedirs(folder_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Don't delete the CD cell!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/turb/data02_02/ncar-high-rate-fixed-dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for timestep_nr in timestep_range:\n",
    "data_xr = xr.open_dataset(raw_ncar_folder_path + \"/jhd.\" + str(timestep_nr).zfill(3) + \".nc\")\n",
    "\n",
    "# Group 3 velocity components together\n",
    "# This fails with Dask bcs. of write permission error on SciServer Job\n",
    "# Never use dask with remote location on this!!\n",
    "merged_velocity = write_tools.merge_velocities(data_xr, dask_local_dir=dask_local_dir\n",
    "                                               , chunk_size_base=chunk_size, use_dask=True, n_dask_workers=n_dask_workers)\n",
    "\n",
    "\n",
    "# Unabbreviate 'e', 'p', 't' variable names\n",
    "merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})\n",
    "\n",
    "\n",
    "dims = [dim for dim in data_xr.dims]\n",
    "dims.reverse() # use (nnz, nny, nnx) instead of (nnx, nny, nnz)\n",
    "\n",
    "# Split 2048^3 into smaller 512^3 arrays\n",
    "smaller_groups, range_list = write_tools.split_zarr_group(merged_velocity, desired_cube_side, dims)\n",
    "\n",
    "# Given up in favor of Ryan's node coloring technique\n",
    "#     z_order = write_tools.morton_order_cube(cube_side=4)\n",
    "node_assignments = write_tools.node_assignment(cube_side=4)\n",
    "\n",
    "\n",
    "cubes = smaller_groups\n",
    "\n",
    "\n",
    "print('Done preparing data. Starting to verify...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_morton_list = [] # Sorting by Morton code to be consistent with Isotropic8192\n",
    "array_cube_side=data_xr['e'].shape[0]\n",
    "\n",
    "\n",
    "for i in range(len(range_list)):            \n",
    "    min_coord = [a[0] for a in range_list[i]]\n",
    "    max_coord = [a[1] - 1 for a in range_list[i]]\n",
    "            \n",
    "    sorted_morton_list.append((write_tools.morton_pack(array_cube_side, min_coord[0], min_coord[1], min_coord[2]), write_tools.morton_pack(array_cube_side, max_coord[0], max_coord[1], max_coord[2])))\n",
    "        # (write_tools.morton_pack(array_cube_side, min_coord[2], min_coord[1], min_coord[0]), write_tools.morton_pack(array_cube_side, max_coord[2], max_coord[1], max_coord[0]))\n",
    "\n",
    "        \n",
    "sorted_morton_list = sorted(sorted_morton_list)\n",
    "sorted_morton_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, instead of writing to disk, read from disk and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_write(q):\n",
    "    while True:\n",
    "        try:\n",
    "            original_512, zarr_512_path = q.get(timeout=10)  # Adjust timeout as necessary\n",
    "\n",
    "            print(f\"Reading Zarr from {zarr_512_path}...\")\n",
    "            zarr_512 = xr.open_zarr(zarr_512_path)\n",
    "\n",
    "            # Compare attributes\n",
    "            assert original_512.attrs == zarr_512.attrs\n",
    "\n",
    "            # Compare each variable's data\n",
    "            for var in original_512.data_vars:\n",
    "                assert np.array_equal(original_512[var].values, zarr_512[var].values)\n",
    "                assert original_512[var].attrs == zarr_512[var].attrs  # Compare attributes of the variable\n",
    "            \n",
    "            print(f\"{zarr_512_path} CORRECT!\")\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        except AssertionError:\n",
    "            print(f\"Data mismatch found for {zarr_512_path}.\")\n",
    "        finally:\n",
    "            q.task_done()\n",
    "\n",
    "            \n",
    "def flatten_3d_list(lst_3d):\n",
    "    return [element for sublist_2d in lst_3d for sublist_1d in sublist_2d for element in sublist_1d]\n",
    "\n",
    "def search_dict_by_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return None  # Value not found in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_node_assgn = flatten_3d_list(node_assignments)\n",
    "\n",
    "mike_dict = {}\n",
    "for i in range(len(range_list)):            \n",
    "    min_coord = [a[0] for a in range_list[i]]\n",
    "    max_coord = [a[1] - 1 for a in range_list[i]]\n",
    "            \n",
    "    mike_dict[dest_folder_name + str(i + 1).zfill(2)] = sorted_morton_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = flatten_3d_list(cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = queue.Queue()\n",
    "\n",
    "\n",
    "# Populate the queue with tasks\n",
    "for i in range(len(range_list)):\n",
    "#     for j in range(4):\n",
    "#         for k in range(4):\n",
    "    min_coord = [a[0] for a in range_list[i]]\n",
    "    max_coord = [a[1] - 1 for a in range_list[i]]\n",
    "    \n",
    "    morton = (write_tools.morton_pack(array_cube_side, min_coord[2], min_coord[1], min_coord[0]), write_tools.morton_pack(array_cube_side, max_coord[2], max_coord[1], max_coord[0]))\n",
    "    \n",
    "    chunk_name = search_dict_by_value(mike_dict, morton)\n",
    "    \n",
    "    idx = int(chunk_name[-2:].lstrip('0'))\n",
    "    \n",
    "    filedb_index = flattened_node_assgn[idx - 1] - 1\n",
    "    \n",
    "    destination = os.path.join(folders[filedb_index], dest_folder_name + str(idx).zfill(2) + \"_\" + str(timestep_nr).zfill(3) + \".zarr\")\n",
    "    \n",
    "    current_array = cubes[i]\n",
    "            \n",
    "    q.put((current_array, destination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create threads and start them\n",
    "\n",
    "threads = []\n",
    "for _ in range(num_threads):\n",
    "    t = threading.Thread(target=verify_write, args=(q,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Wait for all tasks to be processed\n",
    "q.join()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
